{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import argparse\n",
    "import SimpleITK as sitk\n",
    "import random\n",
    "import keras\n",
    "import keras.backend as K\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(602, 602)\n"
     ]
    }
   ],
   "source": [
    "img = sitk.ReadImage(r\"E:\\kits19\\data\\case_00000\\segmentation.nii.gz\")\n",
    "arry = sitk.GetArrayFromImage(img)\n",
    "\n",
    "\n",
    "print(arry[:,:,0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = None\n",
    "\n",
    "def ParseArgs():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"trainingdatafile\", help=\"Input Dataset file for training\")\n",
    "    parser.add_argument(\"modelfile\", help=\"Output trained model file in HDF5 format (*.hdf5).\")\n",
    "    parser.add_argument(\"-t\",\"--testfile\", help=\"Input Dataset file for validation\")\n",
    "    parser.add_argument(\"-e\", \"--epochs\", help=\"Number of epochs\", default=1000, type=int)\n",
    "    parser.add_argument(\"-b\", \"--batchsize\", help=\"Batch size\", default=10, type=int)\n",
    "    parser.add_argument(\"-l\", \"--learningrate\", help=\"Learning rate\", default=1e-3, type=float)\n",
    "    parser.add_argument(\"--nobn\", help=\"Do not use batch normalization layer\", action='store_true')\n",
    "    parser.add_argument(\"--nodropout\", help=\"Do not use dropout layer\", action='store_true')\n",
    "    parser.add_argument(\"--noaugmentation\", help=\"Do not use training data augmentation\", action='store_true')\n",
    "    parser.add_argument(\"--magnification\", help=\"Magnification coefficient for data augmentation\", default=10, type=int)\n",
    "    parser.add_argument(\"--latestfile\", help=\"The filename of the latest weights.\")\n",
    "    parser.add_argument(\"--bestfile\", help=\"The filename of the best weights.\")\n",
    "    parser.add_argument(\"--weightinterval\", help=\"The interval between checkpoint for weight saving.\", type=int)\n",
    "    parser.add_argument(\"--weightfile\", help=\"The filename of the trained weight parameters file for fine tuning or resuming.\")\n",
    "    parser.add_argument(\"--premodel\", help=\"The filename of the previously trained model\")\n",
    "    parser.add_argument(\"--initialepoch\", help=\"Epoch at which to start training for resuming a previous training\", default=0, type=int)\n",
    "    #parser.add_argument(\"--idlist\", help=\"The filename of ID list for splitting input datasets into training and validation datasets.\")\n",
    "    #parser.add_argument(\"--split\", help=\"Fraction of the training data to be used as validation data.\", default=0.0, type=float)\n",
    "    parser.add_argument(\"--logdir\", help=\"Log directory\", default='log')\n",
    "    parser.add_argument(\"-g\", \"--gpuid\", help=\"ID of GPU to be used for segmentation. [default=0]\", default=0, type=int)\n",
    "    parser.add_argument(\"--history\")\n",
    "\n",
    "    args = parser.parse_args()\n",
    "    return args\n",
    "\n",
    "\n",
    "def createParentPath(filepath):\n",
    "    head, _ = os.path.split(filepath)\n",
    "    if len(head) != 0:\n",
    "        os.makedirs(head, exist_ok = True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def GetInputShapes(filenamepair):\n",
    "    image = ImportImage(filenamepair[0])\n",
    "    label = ImportImage(filenamepair[1])\n",
    "    return (image.shape, label.shape)\n",
    "\n",
    "\n",
    "def GetMinimumValue(image):\n",
    "    minmax = sitk.MinimumMaximumImageFilter()\n",
    "    minmax.Execute(image)\n",
    "    return minmax.GetMinimum()\n",
    "\n",
    "\n",
    "def Affine(t, r, scale, shear, c):\n",
    "    a = sitk.AffineTransform(2)\n",
    "    a.SetCenter(c)\n",
    "    a.Scale(scale)\n",
    "    a.Rotate(0,1,r)\n",
    "    a.Shear(0,1,shear[0])\n",
    "    a.Shear(1,0,shear[1])\n",
    "    a.Translate(t)\n",
    "    return a\n",
    "\n",
    "\n",
    "def Transforming(image, bspline, affine, interpolator, minval):\n",
    "    # B-spline transformation\n",
    "    transformed_b = sitk.Resample(image, bspline, interpolator, minval)\n",
    "\n",
    "    # Affine transformation\n",
    "    transformed_a = sitk.Resample(transformed_b, affine, interpolator, minval)\n",
    "\n",
    "    return transformed_a\n",
    "\n",
    "\n",
    "def ImportImageTransformed(imagefile, labelfile):\n",
    "    sigma = 4\n",
    "    translationrange = 5 # [mm]\n",
    "    rotrange = 5 # [deg]\n",
    "    shearrange = 1/16 \n",
    "    scalerange = 0.05\n",
    "\n",
    "    image = sitk.ReadImage(imagefile)\n",
    "    label = sitk.ReadImage(labelfile)\n",
    "\n",
    "    # B-spline parameters\n",
    "    bspline = sitk.BSplineTransformInitializer(image, [5,5])\n",
    "    p = bspline.GetParameters()\n",
    "    numbsplineparams = len(p)\n",
    "    coeff = np.random.normal(0, sigma, numbsplineparams)\n",
    "    bspline.SetParameters(coeff)\n",
    "\n",
    "    # Affine parameters\n",
    "    translation = np.random.uniform(-translationrange, translationrange, 2)\n",
    "    rotation = np.radians(np.random.uniform(-rotrange, rotrange))\n",
    "    shear = np.random.uniform(-shearrange, shearrange, 2)\n",
    "    scale = np.random.uniform(1-scalerange, 1+scalerange)\n",
    "    center = np.array(image.GetSize()) * np.array(image.GetSpacing()) / 2\n",
    "    affine = Affine(translation, rotation, scale, shear, center)\n",
    "\n",
    "    minval = GetMinimumValue(image)\n",
    "\n",
    "    transformed_image = Transforming(image, bspline, affine, sitk.sitkLinear, minval)\n",
    "    transformed_label = Transforming(label, bspline, affine, sitk.sitkNearestNeighbor, 0)\n",
    "\n",
    "    imagearry = sitk.GetArrayFromImage(transformed_image)\n",
    "    imagearry = imagearry[..., np.newaxis]\n",
    "    labelarry = sitk.GetArrayFromImage(transformed_label)\n",
    "\n",
    "    return imagearry, labelarry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ImportBatchArray(datalist, batch_size = 32, apply_augmentation = False):\n",
    "    while True:\n",
    "        indices = list(range(len(datalist)))\n",
    "        random.shuffle(indices)\n",
    "        \n",
    "\n",
    "        if apply_augmentation:\n",
    "            for i in range(0, len(indices), batch_size):\n",
    "                imagelabellist = [ ImportImageTransformed(datalist[idx][0], datalist[idx][1]) for idx in indices[i:i+batch_size] ]\n",
    "                print(\"apply_augmentation\")\n",
    "                imagelist, onehotlabellist = zip(*imagelabellist)\n",
    "                print(\"patch shape1 :\",imagelist.shape)\n",
    "                yield (np.array(imagelist), np.array(onehotlabellist))\n",
    "        else:\n",
    "            for i in range(0, len(indices), batch_size):\n",
    "                imagelist = np.array([ ImportImage(datalist[idx][0]) for idx in indices[i:i+batch_size] ])\n",
    "                \n",
    "                onehotlabellist = np.array([ keras.utils.to_categorical(ImportImage(datalist[idx][1]),num_classes=3) for idx in indices[i:i+batch_size] ])\n",
    "                \n",
    "                yield (imagelist, onehotlabellist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. パスを同じ腎臓ごとにまとめる\n",
    "2. それぞれのまとまりの中で、3つずつに分ける\n",
    "3. 分けたものを3chの配列にする関数\n",
    "4. batcharrayにch数で場合分けして、バッチする"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ImportImage(filename):\n",
    "    image = sitk.ReadImage(filename)\n",
    "    imagearry = sitk.GetArrayFromImage(image)\n",
    "    if image.GetNumberOfComponentsPerPixel() == 1:\n",
    "        imagearry = imagearry[..., np.newaxis]\n",
    "    return imagearry\n",
    "\n",
    "\n",
    "\n",
    "def ImportImage3ch(pList):#[[\"case_00000/image0_00.mha\", \"case_00000/image0_01.mha\", \"case_00000/image0_02.mha\"],\\\n",
    "                          # [\"case_00000/image0_01.mha\", \"case_00000/image0_02.mha\", \"case_00000/image0_03.mha\"]...]\n",
    "    \n",
    "    check = False\n",
    "    for x in pList:\n",
    "        img = sitk.ReadImage(x)\n",
    "        imgArray = sitk.GetArrayFromImage(img)\n",
    "        \n",
    "        if not check:\n",
    "            check = True\n",
    "            stackedArray = imgArray\n",
    "            \n",
    "\n",
    "        else:\n",
    "            stackedArray = np.dstack([stackedArray, imgArray])\n",
    "            \n",
    "\n",
    "    return stackedArray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ReadSliceDataList(filename):\n",
    "    datalist = []\n",
    "    with open(filename) as f:\n",
    "        for line in f:\n",
    "            labelfile, imagefile = line.strip().split('\\t')\n",
    "            datalist.append((imagefile, labelfile))\n",
    "\n",
    "    return datalist\n",
    "\n",
    "def ReadSliceDataList3ch(filename):\n",
    "    datalist = []\n",
    "    with open(filename) as f:\n",
    "        for line in f:\n",
    "            labelfile, imagefile = line.strip().split('\\t')\n",
    "            datalist.append((imagefile, labelfile))\n",
    "            \n",
    "            \n",
    "    pathDicImg = {}#{~/case_00000/image0 : ~/case_00000/image0_00.mha}\n",
    "    pathDicLab = {}#{~/case_00000/image0 : ~/case_00000/label0_00.mha}\n",
    "    pathList = []#3枚ごとにまとめられたリスト(image,label)\n",
    "\n",
    "\n",
    "    #パスを同じ腎臓ごとにまとめる\n",
    "    for path in datalist:\n",
    "\n",
    "        dicPathI, filePathI = os.path.split(path[0])\n",
    "        dicPathL, filePathL = os.path.split(path[1])\n",
    "        \n",
    "        fI,nameI = filePathI.split(\"_\")\n",
    "        fL,nameL = filePathL.split(\"_\")\n",
    "        fPathI = os.path.join(dicPathI, fI)\n",
    "        fPathL = os.path.join(dicPathL, fL)\n",
    "\n",
    "        if fPathI not in pathDicImg:\n",
    "            pathDicImg[fPathI] = []\n",
    "\n",
    "        pathDicImg[fPathI].append(path[0])\n",
    "        \n",
    "        if fPathL not in pathDicLab:\n",
    "            pathDicLab[fPathL] = []\n",
    "\n",
    "        pathDicLab[fPathL].append(path[1])\n",
    "\n",
    "    #同じ腎臓の中で、あるスライスと前後2枚をくっつける(path)\n",
    "\n",
    "    for (keyI, valueI), (keyL, valueL) in zip(pathDicImg.items(), pathDicLab.items()):\n",
    "        for x in range(1,len(valueI)-1):\n",
    "            pathList.append((valueI[x-1:x+2], valueL[x-1:x+2]))\n",
    "   \n",
    "    return pathList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ReadSliceDataList3ch_1ch(filename):\n",
    "    datalist = []\n",
    "    with open(filename) as f:\n",
    "        for line in f:\n",
    "            labelfile, imagefile = line.strip().split('\\t')\n",
    "            datalist.append((imagefile, labelfile))\n",
    "            \n",
    "            \n",
    "    pathDicImg = {}#{~/case_00000/image0 : ~/case_00000/image0_00.mha}\n",
    "    labellist = {}\n",
    "    pathList = []#3枚ごとにまとめられたリスト(image,label)\n",
    "\n",
    "    #パスを同じ腎臓ごとにまとめる\n",
    "    for path in datalist:\n",
    "        dicPathI, filePathI = os.path.split(path[0])\n",
    "        fI,nameI = filePathI.split(\"_\")\n",
    "        fPathI = os.path.join(dicPathI, fI)\n",
    "        \n",
    "\n",
    "        if fPathI not in pathDicImg:\n",
    "            pathDicImg[fPathI] = []\n",
    "            labellist[fPathI] = []\n",
    "\n",
    "        pathDicImg[fPathI].append(path[0])\n",
    "\n",
    "        labellist[fPathI].append(path[1])\n",
    "\n",
    "    #同じ腎臓の中で、あるスライスと前後2枚をくっつける(path)\n",
    "\n",
    "    for (keyI, valueI),(labkey, labvalue) in zip(pathDicImg.items(), labellist.items()):\n",
    "        valueI = sorted(valueI)\n",
    "        labvalue = sorted(labvalue)\n",
    "        for x in range(1,len(valueI)-1):\n",
    "            pathList.append((valueI[x-1:x+2], labvalue[x]))\n",
    "   \n",
    "    return pathList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "pathList = ReadSliceDataList3ch_1ch(\"/Users/tanimotoryou/Documents/Documents/lab/program/2Dkidney/slice/margeTraining_8_2.txt\")\n",
    "for x, y in pathList:\n",
    "    _, a = os.path.split(x[1])\n",
    "    _, b = os.path.split(y)\n",
    "    _, a = a.split(\"_\")\n",
    "    _, b = b.split(\"_\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(['slice/0/image0_03.mha', 'slice/0/image0_03.mha', 'slice/0/image0_03.mha'], 'slice/0/label0_03.mha'), (['slice/0/image0_03.mha', 'slice/0/image0_03.mha', 'slice/0/image0_03.mha'], 'slice/0/label0_03.mha'), (['slice/0/image0_03.mha', 'slice/0/image0_03.mha', 'slice/0/image0_02.mha'], 'slice/0/label0_03.mha'), (['slice/0/image0_03.mha', 'slice/0/image0_02.mha', 'slice/0/image0_02.mha'], 'slice/0/label0_02.mha'), (['slice/0/image0_02.mha', 'slice/0/image0_02.mha', 'slice/0/image0_02.mha'], 'slice/0/label0_02.mha'), (['slice/0/image0_02.mha', 'slice/0/image0_02.mha', 'slice/0/image0_02.mha'], 'slice/0/label0_02.mha'), (['slice/0/image0_02.mha', 'slice/0/image0_02.mha', 'slice/0/image0_01.mha'], 'slice/0/label0_02.mha'), (['slice/0/image0_02.mha', 'slice/0/image0_01.mha', 'slice/0/image0_01.mha'], 'slice/0/label0_01.mha'), (['slice/0/image0_01.mha', 'slice/0/image0_01.mha', 'slice/0/image0_01.mha'], 'slice/0/label0_01.mha'), (['slice/0/image0_01.mha', 'slice/0/image0_01.mha', 'slice/0/image0_01.mha'], 'slice/0/label0_01.mha'), (['slice/0/image0_01.mha', 'slice/0/image0_01.mha', 'slice/0/image0_00.mha'], 'slice/0/label0_01.mha'), (['slice/0/image0_01.mha', 'slice/0/image0_00.mha', 'slice/0/image0_00.mha'], 'slice/0/label0_00.mha'), (['slice/0/image0_00.mha', 'slice/0/image0_00.mha', 'slice/0/image0_00.mha'], 'slice/0/label0_00.mha'), (['slice/0/image0_00.mha', 'slice/0/image0_00.mha', 'slice/0/image0_00.mha'], 'slice/0/label0_00.mha')]\n",
      "(256, 256, 3)\n",
      "(256, 256, 3)\n",
      "(256, 256, 3)\n",
      "(256, 256, 3)\n",
      "(256, 256, 3)\n",
      "(256, 256, 3)\n",
      "(256, 256, 3)\n",
      "(256, 256, 3)\n",
      "(256, 256, 3)\n",
      "(256, 256, 3)\n",
      "(256, 256, 3)\n",
      "(256, 256, 3)\n",
      "(256, 256, 3)\n",
      "(256, 256, 3)\n"
     ]
    }
   ],
   "source": [
    "pathList = ReadSliceDataList3ch_1ch(\"slice/0.txt\")\n",
    "print(pathList)\n",
    "\n",
    "for p in pathList:\n",
    "    array = ImportImage3ch(p[0])\n",
    "    print(array.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "def To3chDataList(dataPath):\n",
    "    pathDic = {}#{~/case_00000/image0 : ~/case_00000/image0_00.mha}\n",
    "    pathList = []#3枚ごとにまとめられたリスト\n",
    "    \n",
    "    #パスを同じ腎臓ごとにまとめる\n",
    "    for path in dataPath:\n",
    "\n",
    "        dicPath, filePath = os.path.split(path)\n",
    "        f,name = filePath.split(\"_\")\n",
    "        fPath = os.path.join(dicPath, f)\n",
    "\n",
    "        if fPath not in pathDic:\n",
    "            pathDic[fPath] = []\n",
    "\n",
    "        pathDic[fPath].append(path)\n",
    "    print(pathDic)\n",
    "    #同じ腎臓の中で、あるスライスと前後2枚をくっつける(path)\n",
    "\n",
    "    for key, value in pathDic.items():\n",
    "        for x in range(1,len(value)-1):\n",
    "            pathList.append(value[x-1:x+2])\n",
    "    \n",
    "    return pathList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'To3chDataList' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-65-2ad630c7dfc4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdatalist\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mpathList\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTo3chDataList\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpathList\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpathList\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'To3chDataList' is not defined"
     ]
    }
   ],
   "source": [
    "datalist = ReadSliceDataList(\"/Users/tanimotoryou/Documents/Documents/lab/program/2Dkidney/slice/2.txt\")\n",
    "\n",
    "for d in datalist:\n",
    "    pathList = To3chDataList(d[0])\n",
    "print(pathList)\n",
    "for p in pathList:\n",
    "    array = ImportImage3ch(p)\n",
    "    print(array.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(_):\n",
    "    \n",
    "<<<<<<< HEAD\n",
    "    experiment = Experiment(api_key=\"IowbTppLPOohqhcDtzxw76Cot\")#comet_mlに保存\n",
    "    comet_callback = experiment.get_keras_callback()\n",
    "=======\n",
    "    \n",
    ">>>>>>> 48cc187a15e8e5fb05b6fbaffbbd81c63ea69a6e\n",
    "                        \n",
    "    config = tf.ConfigProto()\n",
    "    config.gpu_options.allow_growth = True\n",
    "    config.allow_soft_placement = True\n",
    "    sess = tf.Session(config=config)\n",
    "    tf.keras.backend.set_session(sess)\n",
    "\n",
    "    trainingdatalist = ReadSliceDataList(args.trainingdatafile)\n",
    "    testdatalist = None\n",
    "    if args.testfile is not None:\n",
    "        testdatalist = ReadSliceDataList(args.testfile)\n",
    "        testdatalist = random.sample(testdatalist, int(len(testdatalist)*0.1))\n",
    "\n",
    "    (imageshape, labelshape) = GetInputShapes(trainingdatalist[0])\n",
    "    nclasses = 3 # Number of classes\n",
    "    print(imageshape)\n",
    "    print(labelshape)\n",
    "\n",
    "    with tf.device('/device:GPU:{}'.format(args.gpuid)):\n",
    "        x = tf.keras.layers.Input(shape=imageshape, name=\"x\")\n",
    "        segmentation = ConstructModel(x, nclasses, not args.nobn, not args.nodropout)\n",
    "        model = tf.keras.models.Model(x, segmentation)\n",
    "        model.summary()\n",
    "\n",
    "        optimizer = tf.keras.optimizers.Adam(lr=args.learningrate)\n",
    "\n",
    "        model.compile(loss=penalty_categorical, optimizer=optimizer, metrics=[kidney_dice, cancer_dice])\n",
    "\n",
    "    createParentPath(args.modelfile)\n",
    "    with open(args.modelfile, 'w') as f:\n",
    "        f.write(model.to_yaml())\n",
    "\n",
    "    if args.weightfile is None:\n",
    "        initial_epoch = 0\n",
    "    else:\n",
    "        model.load_weights(args.weightfile)\n",
    "        initial_epoch = args.initialepoch\n",
    "\n",
    "    if args.latestfile is None:\n",
    "        latestfile = args.logdir + '/latestweights.hdf5'\n",
    "    else:\n",
    "        latestfile = args.latestfile\n",
    "        createParentPath(latestfile)\n",
    "\n",
    "    tb_cbk = tf.keras.callbacks.TensorBoard(log_dir=args.logdir)\n",
    "    latest_cbk = LatestWeightSaver(latestfile)\n",
    "    callbacks = [tb_cbk, latest_cbk]\n",
    "    if testdatalist is not None:\n",
    "        if args.bestfile is None:\n",
    "            bestfile = args.logdir + '/bestweights.hdf5'\n",
    "        else:\n",
    "            bestfile = args.bestfile\n",
    "            createParentPath(bestfile)\n",
    "        chkp_cbk = tf.keras.callbacks.ModelCheckpoint(filepath=bestfile, save_best_only = True, save_weights_only = True)\n",
    "        callbacks.append(chkp_cbk)\n",
    "    if args.weightinterval is not None:\n",
    "        periodic_cbk = PeriodicWeightSaver(logdir=args.logdir, interval=args.weightinterval)\n",
    "        callbacks.append(periodic_cbk)\n",
    "\n",
    "    steps_per_epoch = len(trainingdatalist) / args.batchsize \n",
    "    print (\"Batch size: {}\".format(args.batchsize))\n",
    "    print (\"Number of Epochs: {}\".format(args.epochs))\n",
    "    print (\"Number of Steps/epoch: {}\".format(steps_per_epoch))\n",
    "\n",
    "    with tf.device('/device:GPU:{}'.format(args.gpuid)):\n",
    "        if testdatalist is not None:\n",
    "            historys = model.fit_generator(ImportBatchArray(trainingdatalist, batch_size = args.batchsize, apply_augmentation = False),\n",
    "                    steps_per_epoch = int(steps_per_epoch), epochs = args.epochs,\n",
    "                    callbacks=callbacks,\n",
    "                    validation_data = ImportBatchArray(testdatalist, batch_size = args.batchsize),\n",
    "                    validation_steps = len(testdatalist),\n",
    "                    initial_epoch = int(initial_epoch))\n",
    "        else:\n",
    "            historys = model.fit_generator(ImportBatchArray(trainingdatalist, batch_size = args.batchsize, apply_augmentation = False),\n",
    "                    steps_per_epoch = int(steps_per_epoch), epochs = args.epochs,\n",
    "                    callbacks=callbacks,\n",
    "                    initial_epoch = int(initial_epoch))\n",
    "\n",
    "    \n",
    "    loss = historys.history['dice']\n",
    "    val_loss = historys.history['val_dice']\n",
    "    epochs = len(loss)\n",
    "    \n",
    "    history_file = open(args.history,\"a\")\n",
    "\n",
    "    for x in range(epochs):\n",
    "        print(\"{}\\t{}\".format(loss[x],val_loss[x]),file = history_file)\n",
    "    print(\"\\n\",file=history_file)\n",
    "        \n",
    "    history_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    args = ParseArgs()\n",
    "    t1 = time.time()\n",
    "\n",
    "    tf.app.run(main=main, argv=[sys.argv[0]])\n",
    "\n",
    "    t2 = time.time()\n",
    "    caluculateTime(t1, t2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
